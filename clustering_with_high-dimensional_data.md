# Clustering with high-dimensional data

![](pics/intro.png)


Clustering aims to group data points together so that within the same group the observations are very similar while between different groups the observations are very different, according to some dissimilarity rule.

**Note: The data points do not need to come with any labels.** 

Often, clustering is used as a way to decide what and how many class labels are suitable for the data set. 

**For example, we may want to cluster gene expression data into groups that will characterize different cell types in a blood sample.**

Other examples:
  - Find customer groups to adjust advertisement
  - Find subtypes of diseases to fine-tune treatment

In a na√Øve approach we may try to assign data to clusters using *brute force* methods. However, we quickly find that it is impossible to search through all assignments as it scales with $k^N$, where $k$ is the number of clusters and $N$ the sample size.

Therefore we need smarter approaches to this problem.

## K-means

K means start with a pre-defined number of clusters $K$ and aims at minimizing the value of the **within group sum of squares (WGSS)**:

$$
WGSS = \sum_{k=1}^N\sum_{x^{{i}},x^{(j)} \in C_k} |x^{(i)}-x^{(j)}|^2,
$$

where the $k$ indexes the K different clusters, $C_k$ denotes the $k$-th cluster and $|x^{(i)}-x^{(j)}|$ is the euclidean distance between the data points $i$ and $j$.

The WGSS measures how dissimilar data points in the same cluster are. To find an algorithm to minimize WGSS, it helps to first rewrite WGSS in terms of the means $\mu_k = \sum_{x^{(i)} \in C_k} x^{(i)}/n_k$ of each cluster $C_k$.

$$
WGSS = \sum_{k=1}^N\sum_{x^{{i}},x^{(j)} \in C_k} |(x^{(i)}-\mu_k) - (x^{(j)}-\mu_k)|^2 = \cdots \\
= 2 \sum_{k=1}^K |(x^{(i)}-\mu_k)|^2.
$$

### K-means algorithm

1. Initialize the K means ${\mu_k}_k=1,...,K to random positions.

Then repeat 2. and 3. until convergence.

2. Cluster assignment. Cluster each point with the closest centroid $\mu_k$. Call the set of all points in the cluster $C_k$.
3. Centroids update. Update all centroids $\mu_k$ to be the average position of all points assigned to $C_k$ in the step above.

**Note: K-means does not guarantee convergence to the global minimum. Therefore it is necessary to conduct multiple runs starting with different random initializations. This is more effective when the number of clusters is small.**

### K-medoids

There are two limitations with K-means:
- Its results are sensitive to outliers.
- The cluster centroids are not necessarily data points

In order to overcome these issues, it is possible no modify the algorithm and use medoids instead of means. The medoid is just the data point of a cluster closest to its mean.

### Optimizing K

One heuristic method to determine the number of clusters K is the **elbow method**. 

Plot the loss function WGSS as a function of K and pick the K corresponding to the "elbow" of the plot. For instance, according to the plot below, the best choice for K is 3. Basically what is important here is to balance how much information each cluster provides is with the number of clusters: after some number $k$ the new information gained is very small.

![](pics/elbow.png)

## Gaussian mixture models

Clustering using Gaussian mixture model (GMM) generalizes $K$-mean in two ways:

- Cluster assignment is based on the probabilities of the data point being generated by the different clusters.

- The shape of the clusters can be elliptical rather than only spherical.

Consider the Gaussian mixture model of $K$ Gaussians. We can write the probability of obtaining the observation $\mathbf{X}$ as

$$
P(\mathbf{X}) = \sum_{k=1}^K p_k P(\mathbf{X}|cluster\ k)
$$

where

$$
p_k = P(cluster\ k)
$$

is the missing proportion of each cluster, and

$$
\mathbf{X}|cluster\ k \sim \mathcal{N}(\mu_k,\Sigma_k). 
$$

$P(X|cluster\ k)$ is the probability of obtaining the observation $\mathbf{X}$ given that it is generated by the model for cluster $k$. This mixture has parameters $\theta = \{p1,...,p_k,\mu_1,...,\mu_K,\Sigma_1,..,\Sigma_K\}$. They correspond to the missing proportions, means and covariance matrices of each of the $K$ gaussians respectively.

